<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>GPU remap in tensorflow | moontree’s blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="GPU remap in tensorflow" />
<meta name="author" content="moontree" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="训练tensorflow模型的时候，可以通过tf.device(&#39;/gpu:0&#39;)这种方式来为图节点分配不同的gpu。 那么，在推理阶段，该如何将不同的模型分配到不同的gpu上去执行呢？ 比如这种场景，有4个模型，每个模型占据6G显存，这样的话，它们必须被分配到不同的GPU上才不会导致显存溢出。" />
<meta property="og:description" content="训练tensorflow模型的时候，可以通过tf.device(&#39;/gpu:0&#39;)这种方式来为图节点分配不同的gpu。 那么，在推理阶段，该如何将不同的模型分配到不同的gpu上去执行呢？ 比如这种场景，有4个模型，每个模型占据6G显存，这样的话，它们必须被分配到不同的GPU上才不会导致显存溢出。" />
<link rel="canonical" href="http://localhost:4000/tensorflow-usage/2019/01/12/gpu-remap-in-tensorflow.html" />
<meta property="og:url" content="http://localhost:4000/tensorflow-usage/2019/01/12/gpu-remap-in-tensorflow.html" />
<meta property="og:site_name" content="moontree’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-12T00:00:00+08:00" />
<script type="application/ld+json">
{"headline":"GPU remap in tensorflow","dateModified":"2019-01-12T00:00:00+08:00","datePublished":"2019-01-12T00:00:00+08:00","url":"http://localhost:4000/tensorflow-usage/2019/01/12/gpu-remap-in-tensorflow.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/tensorflow-usage/2019/01/12/gpu-remap-in-tensorflow.html"},"author":{"@type":"Person","name":"moontree"},"description":"训练tensorflow模型的时候，可以通过tf.device(&#39;/gpu:0&#39;)这种方式来为图节点分配不同的gpu。 那么，在推理阶段，该如何将不同的模型分配到不同的gpu上去执行呢？ 比如这种场景，有4个模型，每个模型占据6G显存，这样的话，它们必须被分配到不同的GPU上才不会导致显存溢出。","@type":"BlogPosting","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="moontree's blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">moontree&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">GPU remap in tensorflow</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-01-12T00:00:00+08:00" itemprop="datePublished">Jan 12, 2019
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">moontree</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>训练tensorflow模型的时候，可以通过<code class="highlighter-rouge">tf.device('/gpu:0')</code>这种方式来为图节点分配不同的gpu。
那么，在推理阶段，该如何将不同的模型分配到不同的gpu上去执行呢？
比如这种场景，有4个模型，每个模型占据6G显存，这样的话，它们必须被分配到不同的GPU上才不会导致显存溢出。</p>

<p>然而，在搜索”tensorflow分配gpu”的时候，通常会出现3种结果：</p>
<ul>
  <li>训练时指定，也就是上述方式</li>
  <li>命令行指定，<code class="highlighter-rouge">CUDA_VISIBLE_DEVICES=0,1 python *.py</code>，然而，这种方式只能防止tensorflow使用其他的gpu，并不能将4个模型指定到不同的设备上</li>
  <li>代码中指定，<code class="highlighter-rouge">os.environ['CUDA_VISIBLE_DEVCIES']=0</code>，然而，这种方式也有问题，不管指定多少次，最终只会使用一个gpu，具体哪个和实际分配的顺序有关</li>
</ul>

<p>看起来，上述三种方式中，后面两种都不能满足我的需求。
第一种应该是可行的，但是似乎有些不够优雅。有没有更优雅的方式呢？</p>

<p>考虑到之前，为了限制gpu的内存使用，
我们会在创建Session的时候加以限制：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gpu_options</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GPUOptions</span><span class="p">(</span><span class="n">allow_growth</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span><span class="n">allow_soft_placement</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">gpu_options</span><span class="o">=</span><span class="n">gpu_options</span><span class="p">))</span>
</code></pre></div></div>
<p>看来在<code class="highlighter-rouge">tf.GPUOptions</code>里面，可以对gpu做限制，很好，来看一下<a href="https://www.tensorflow.org/api_docs/python/tf/GPUOptions">官方文档</a>，
很简单的描述，看来要去<a href="https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/core/protobuf/config.proto">代码</a>里查看了。
搜索关键字<code class="highlighter-rouge">device</code>，发现了如下描述：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  // A comma-separated list of GPU ids that determines the 'visible'
  // to 'virtual' mapping of GPU devices.  For example, if TensorFlow
  // can see 8 GPU devices in the process, and one wanted to map
  // visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
  // then one would specify this field as "5,3".  This field is similar in
  // spirit to the CUDA_VISIBLE_DEVICES environment variable, except
  // it applies to the visible GPU devices in the process.
  //
  // NOTE:
  // 1. The GPU driver provides the process with the visible GPUs
  //    in an order which is not guaranteed to have any correlation to
  //    the *physical* GPU id in the machine.  This field is used for
  //    remapping "visible" to "virtual", which means this operates only
  //    after the process starts.  Users are required to use vendor
  //    specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
  //    physical to visible device mapping prior to invoking TensorFlow.
  // 2. In the code, the ids in this list are also called "platform GPU id"s,
  //    and the 'virtual' ids of GPU devices (i.e. the ids in the device
  //    name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
  //    refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
  //    for more information.
  string visible_device_list = 5;
</code></pre></div></div>
<p>看起来不错，似乎只要在gpuoptions里加上一个参数就可以了。
代码调整如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gpu_options_1</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GPUOptions</span><span class="p">(</span><span class="n">allow_growth</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">visible_device_list</span><span class="o">=</span><span class="s">'1'</span><span class="p">)</span>
<span class="n">session1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span><span class="n">allow_soft_placement</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">gpu_options</span><span class="o">=</span><span class="n">gpu_options_1</span><span class="p">))</span>
<span class="n">gpu_options_2</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GPUOptions</span><span class="p">(</span><span class="n">allow_growth</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">visible_device_list</span><span class="o">=</span><span class="s">'2'</span><span class="p">)</span>
<span class="n">session2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span><span class="n">allow_soft_placement</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">gpu_options</span><span class="o">=</span><span class="n">gpu_options_2</span><span class="p">))</span>
</code></pre></div></div>

<p>看起来很完美！
然而，错误总在你猝不及防的时候出现——</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">framework</span><span class="o">.</span><span class="n">errors_impl</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">:</span>
<span class="s">'visible_device_list'</span> <span class="n">listed</span> <span class="n">an</span> <span class="n">invalid</span> <span class="n">GPU</span> <span class="nb">id</span> <span class="s">'2'</span> <span class="n">but</span> <span class="n">visible</span> <span class="n">device</span> <span class="n">count</span> <span class="ow">is</span> <span class="mi">1</span>
</code></pre></div></div>
<p>看起来很奇怪啊，用法没有问题，为什么就报错了呢……</p>

<p>带着求(bao)知(zha)的心态，去搜索这个错误，居然只有两个帖子……
还只有<a href="https://github.com/tensorflow/tensorflow/issues/18861">一个</a>有效……
不过还好，毕竟有人遇到过同样的问题，那就是个好消息~
然后看到了一个不幸的回复：<code class="highlighter-rouge">gpuoptions</code>是对整个进程生效的。原文如下：</p>
<blockquote>
  <p>Sorry for the confusion, let me clarify:</p>
</blockquote>

<blockquote>
  <p>it’s totally fine to use different configs in different process,
  there is no restriction on that.
  For example, you may set CUDA_VISIBLE_DEVICES=1 in one process and 2 in another,
  you may also set visible_gpu_device or any other ConfigProto options differently.
  in the same process, if possible,
  we should use same GPUOptions for all sessions,
  as most of the options inside GPUOptions are process-wide (AFAIK,
  per_process_gpu_memory_fraction, allocator_type, allow_growth, visible_device_list, experimental are all process-wide options).
  If we use different GPUOptions for different sessions, unexpected behavior may/would occur, depending on which options you set differently.
  In your case, specifically for the code:</p>
</blockquote>

<blockquote>
  <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  G =tf.Graph()
  sess1 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='0')))
  sess2 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='1')))
</code></pre></div>  </div>
</blockquote>

<blockquote>
  <p><code class="highlighter-rouge">sess1</code> and <code class="highlighter-rouge">sess2</code> are in same process and using same graph but different GPUOptions options (visible_device_list is different).</p>
  <ul>
    <li>For <code class="highlighter-rouge">sess1</code>, a <code class="highlighter-rouge">BaseGPUDevice</code> will be created with name ‘/gpu:0’ and pointing to physical gpu 0</li>
    <li>For <code class="highlighter-rouge">sess2</code>, a <code class="highlighter-rouge">BaseGPUDevice</code> will be created with name ‘/gpu:0’ but pointing to physical gpu 1.</li>
  </ul>
</blockquote>

<blockquote>
  <p>Note that both device have same name ‘/gpu:0’,
  so any code that use only the device name (not the BaseGPUDevice object itself)
  to access the physical gpu will be directed to physical gpu 0,
  this is what I called ‘unexpected behavior’.
  For example, assume that G has a node placed in ‘/gpu:0’,
  grappler will use the information from physical gpu 0 to optimize your graph in sess2 which is not what we want.</p>
</blockquote>

<blockquote>
  <p>In order to apply the same graph to different gpu,
 we can use with tf.device() with different device name
 when building/importing the graph.</p>
</blockquote>

<blockquote>
  <p>Thanks.</p>
</blockquote>

<p>哈哈，本宝宝很开心……这居然是一个单进程的设置，会影响session的创建。看了看后续的解释，还是很有道理的。
为了避免冲突，同一进程中的session要用相同的设置，
但是可以把不同的graph放到不同的gpu上执行。
后面给出了一个示例：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">google.protobuf</span> <span class="kn">import</span> <span class="n">text_format</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">hello</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s">'Hello, TensorFlow!'</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">write_graph</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">(),</span> <span class="s">'/tmp'</span><span class="p">,</span> <span class="s">'g1'</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'/tmp/g1'</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
  <span class="n">graph_text</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">graph_text</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'-'</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">graphs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
  <span class="n">graphs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">())</span>
  <span class="k">with</span> <span class="n">graphs</span><span class="p">[</span><span class="n">g</span><span class="p">]</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">graph_def</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">GraphDef</span><span class="p">()</span>
    <span class="n">text_format</span><span class="o">.</span><span class="n">Merge</span><span class="p">(</span><span class="n">graph_text</span><span class="p">,</span> <span class="n">graph_def</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'/gpu:'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">g</span><span class="p">)):</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">import_graph_def</span><span class="p">(</span><span class="n">graph_def</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">''</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">as_graph_def</span><span class="p">())</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'-'</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>

</code></pre></div></div>
<p>貌似可行，但总感觉不够优雅。注意到上述关键字，把不同的图放到不同的gpu上执行，那么，是不是<code class="highlighter-rouge">tf.Graph</code>本身就支持这样的操作呢？
之前并没有去了解过这个东西：</p>

<h2 id="tfgraph-and-tfsession">tf.Graph and tf.Session</h2>

<h3 id="tfgraph">tf.Graph</h3>
<p>tensorflow的计算被表示为数据流图。
一个<code class="highlighter-rouge">Graph</code>包含了一系列的操作（计算单元，<code class="highlighter-rouge">tf.Operator</code>)，以及张量（数据单元，<code class="highlighter-rouge">tf.Tensor</code>）</p>

<p>通常会有一个默认的图，可以通过<code class="highlighter-rouge">tf.get_default_graph</code>来查看。</p>

<p>可以通过<code class="highlighter-rouge">tf.graph.device('/device:GPU:0')</code>来指定设备：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">g</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'/device:GPU:0'</span><span class="p">):</span>
  <span class="c1"># All operations constructed in this context will be placed
</span>  <span class="c1"># on GPU 0.
</span>  <span class="k">with</span> <span class="n">g</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">None</span><span class="p">):</span>
    <span class="c1"># All operations constructed in this context will have no
</span>    <span class="c1"># assigned device.
</span>    <span class="k">pass</span>
</code></pre></div></div>

<h3 id="tfsession">tf.Session</h3>
<blockquote>
  <p>A class for running TensorFlow operations.</p>
</blockquote>

<blockquote>
  <p>A Session object encapsulates the environment in which Operation objects are executed, and Tensor objects are evaluated. For example:</p>
</blockquote>

<p>Session可以捕获操作的环境，并且去运行得到tensor的值。</p>

<p>仔细看下初始化函数，</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__init__</span><span class="p">(</span>
    <span class="n">target</span><span class="o">=</span><span class="s">''</span><span class="p">,</span>
    <span class="n">graph</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
<span class="s">"""
target: (Optional.) The execution engine to connect to. Defaults to using an in-process engine. See Distributed TensorFlow for more examples.
graph: (Optional.) The Graph to be launched (described above).
config: (Optional.) A ConfigProto protocol buffer with configuration options for the session.
"""</span>
</code></pre></div></div>
<p>很明确了，如果在graph中指定了设备，在session初始化时指定为这个graph，就可以实现之前的目的了。</p>

<p>重点：
graph指定了运行时的设备、操作、张量；而session只是按照graph的指定去运行，本身没有指定设备的功能。</p>

<p>最终示例代码如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">100</span> <span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">])</span>
<span class="n">g1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">g2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">()</span>
<span class="n">c</span><span class="o">.</span><span class="n">allow_soft_placement</span><span class="o">=</span><span class="bp">True</span>
<span class="n">c</span><span class="o">.</span><span class="n">gpu_options</span><span class="o">.</span><span class="n">allow_growth</span><span class="o">=</span><span class="bp">True</span>
<span class="k">with</span> <span class="n">g1</span><span class="o">.</span><span class="n">as_default</span><span class="p">(),</span> <span class="n">g1</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'/device:GPU:0'</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="k">with</span> <span class="n">g2</span><span class="o">.</span><span class="n">as_default</span><span class="p">(),</span> <span class="n">g2</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'/device:GPU:1'</span><span class="p">):</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="n">sess1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">g1</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
<span class="n">sess2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">g2</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
</code></pre></div></div>


  </div><a class="u-url" href="/tensorflow-usage/2019/01/12/gpu-remap-in-tensorflow.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">moontree&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">moontree&#39;s blog</li><li><a class="u-email" href="mailto:313249726@qq.com">313249726@qq.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/moontree"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">moontree</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Swimming in the ocean of code.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
